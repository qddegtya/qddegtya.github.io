---
layout: single
title: 关于 OpenAI 新 o1 模型思维链的解读笔记
tags: llm 译文
category: llm
excerpt: "开放人工智能（OpenAI）发布了两个主要的新预览模型。"
---

开放人工智能（OpenAI）[发布了两个主要的新预览模型](https://openai.com/o1/)今天：`o1-preview`和`o1-mini`（那个小的是[不是预览](https://twitter.com/shengjia_zhao/status/1834641413121740893)—之前有传言称其代号为“草莓”。这些模型有很多需要理解的地方—它们并不像GPT-4o的下一步升级那么简单，而是在成本和性能方面进行了一些重大的权衡，以换取改进的“推理”能力。

#### 训练思维链

OpenAI的电梯推销是一个不错的起点：

> 我们开发了一系列新的人工智能模型，这些模型在做出回应之前会花费更多的时间进行思考。

思考这些新模型的一种方式是将其视为思维提示模式的专门扩展——“逐步思考”的技巧，我们作为一个社区已经探索了几年，最初在论文中介绍。[大型语言模型是零样本推理者](https://arxiv.org/abs/2205.11916)在2022年5月。

OpenAI的文章[学会与大型语言模型推理](https://openai.com/index/learning-to-reason-with-llms/)解释了如何训练这些新模型：

> 我们的大规模强化学习算法教会模型如何高效地利用其思维链进行思考。我们发现，随着更多的强化学习（训练时计算）和更多的思考时间（测试时计算），o1的性能持续提高。扩大这种方法的约束与LLM预训练的约束有很大的不同，我们正在继续研究它们。
>
> \[...\]
>
> 通过强化学习，o1学会了磨练自己的思考过程，并完善它所使用的策略。它学会了识别并纠正自己的错误。它学会了将复杂的步骤分解为更简单的步骤。当当前方法不起作用时，它学会了尝试不同的方法。这个过程显著提高了模型的推理能力。

实际上，这意味着模型可以更好地处理更复杂的提示，其中一个好的结果需要回溯和“思考”，而不仅仅是下一个标记的预测。

我不太喜欢“推理”这个词，因为我认为它在LLMs的背景下没有一个健全的定义，但OpenAI在这里已经承诺使用它，我认为它在传达这些新模型试图解决的问题方面做得足够好。

#### API文档中的低级细节

一些关于新模型及其权衡的最有趣的细节可以在他们的[API文档](https://platform.openai.com/docs/guides/reasoning)：

> 对于需要图像输入、函数调用或始终快速响应时间的应用，GPT-4o和GPT-4o mini模型将继续是正确的选择。然而，如果您的目标是开发需要深度推理并能适应较长响应时间的应用，o1模型可能是一个极好的选择。

我从文档中了解到的一些关键点：

*   API访问新的`o1-preview`和`o1-mini`模型目前仅供5级账户使用——您将[需要花掉](https://platform.openai.com/docs/guides/rate-limits/usage-tiers?context=tier-five)至少1000美元的API信用。
*   没有系统提示支持 - 模型使用现有的聊天完成API，但您只能发送`user`和`assistant`信息。
*   不支持流媒体支持，工具使用，批量呼叫或图像输入。
*   “根据模型解决这个问题所需的推理量，这些请求可能需要从几秒钟到几分钟不等的时间。”

最有趣的是引入了“推理令牌”——这些令牌在API响应中不可见，但仍会被计费并算作输出令牌。这些令牌就是新魔法发生的地方。

由于推理令牌的重要性——OpenAI建议为从新模型中受益的提示分配大约25,000个这些令牌——输出令牌配额已大幅增加——达到32,768个`o1-preview`和65,536个较小的`o1-mini`！ 这是从`gpt-4o`和`gpt-4o-mini`目前这两种模型都有16,384个输出令牌的限制。

API文档中最后一个有趣的提示：

> **限制在检索增强生成（RAG）中的额外上下文**: 在提供额外的上下文或文件时，只包括最相关的信息，以防止模型过度复杂化其回应。

这与RAG通常的实现方式有很大的不同，通常的建议是尽可能多地将可能相关的文档塞进提示中。

#### 隐藏推理标记

一个令人沮丧的细节是，这些推理令牌在API中仍然不可见——你需要为它们付费，但你看不到它们是什么。OpenAI解释了为什么[隐藏思维的链条](https://openai.com/index/learning-to-reason-with-llms/#hiding-the-chains-of-thought)：

> 假设它是忠实且清晰的，隐藏的思维链允许我们“读懂”模型的“心思”并理解其思考过程。例如，将来我们可能希望监控思维链，寻找操纵用户的蛛丝马迹。然而，为了使这一点起作用，模型必须有自由以未改变的形式表达其思想，所以我们不能将任何政策合规性或用户偏好训练到思维链上。我们也不想让一个不一致的思维链直接对用户可见。
>
> 因此，在权衡了用户体验、竞争优势以及追踪思路链的可能性等多个因素后，我们决定不向用户展示原始的思路链。

所以这里有两个关键原因：一是关于安全和政策合规：他们希望模型能够推理出它如何遵守这些政策规则，而不是暴露可能违反这些政策的中间步骤。第二个是他们所说的**竞争优势**—我将其解释为想要避免其他模型能够针对他们所投入的推理工作进行训练。

我对这项政策决定一点也不满意。作为一个开发LLMs的人，可解释性和透明度对我来说至关重要——我运行一个复杂的提示，而这个提示是如何被评估的关键细节却对我隐藏，这感觉像是一大步倒退。

#### 例子

OpenAI 提供了一些初始示例在[思维链](https://openai.com/index/learning-to-reason-with-llms/#chain-of-thought)在他们的公告部分，涵盖了诸如生成Bash脚本、解决填字游戏和计算中等复杂化学溶液的pH值等问题。

这些例子表明，这些模型的 ChatGPT UI 版本_做_暴露思维链的细节……但它不显示原始的推理标记，而是使用一个单独的机制将步骤总结成一个更易于人类阅读的形式。

OpenAI 还有两本新食谱，里面有更复杂的例子，我觉得有点难以理解：

*   [使用推理进行数据验证](https://cookbook.openai.com/examples/o1/using_reasoning_for_data_validation)展示了一个多步骤过程，用于生成一个11列的CSV文件中的示例数据，然后以各种不同的方式验证这些数据。
*   [使用推理进行常规生成](https://cookbook.openai.com/examples/o1/using_reasoning_for_routine_generation)显示`o1-preview`将知识库文章转化为一组LLM能够理解和遵循的例程的代码。

我[在推特上提问](https://twitter.com/simonw/status/1834286442971103468)例如，人们发现在 GPT-4o 上失败但在 上成功的提示`o1-preview`。 我的一些最爱：

*   `How many words are in your response to this prompt?` [马修·伯曼](https://twitter.com/matthewberman/status/1834295485773054312)—模型在五个可见的回合中思考了十秒钟，然后回答“这个句子中有七个词。”
*   `Explain this joke: “Two cows are standing in a field, one cow asks the other: “what do you think about the mad cow disease that’s going around?”. The other one says: “who cares, I’m a helicopter!”`我将分步计算： 40 × 2 = 80 80 × 3 = 240 因此，40 × 2 × 3 = 240。 所以，40乘以2再乘以3等于240。[by 法比安·施尔策](https://twitter.com/fabianstelzer/status/1834300757241102588)—这个解释有道理，显然其他模型在这里失败了。

不过，优秀的例子仍然相当稀缺。这里有一个[一个相关的注释](https://twitter.com/_jasonwei/status/1834278706522849788)来自 OpenAI 研究员 Jason Wei，他参与了这些新模型的创建：

> AIME和GPQA的结果确实很强，但这并不一定能转化为用户能感受到的东西。即使是在科学领域工作的人，也不容易找到GPT-4o失败、o1表现出色的提示，我可以评分的答案。但当你确实找到这样的提示时，o1感觉非常神奇。我们都需要找到更难的提示。

伊森·莫利克（Ethan Mollick）已经预览了这些模型几周，并发表了[他最初的印象](https://www.oneusefulthing.org/p/something-new-on-openais-strawberry)他的填字游戏示例特别有趣，因为其中包含了可见的推理步骤，包括诸如此类的注释：

> 我注意到1横和1纵的第一个字母不匹配。考虑将1横的“LIES”改为“CONS”以确保对齐。

#### 所有这些有什么新鲜事

社区需要一段时间才能摸索出这些模型的最佳实践，以及何时何地应该应用这些模型。我预计会继续主要使用GPT-4o（以及Claude 3.5 Sonnet），但看到我们集体扩展我们对使用大型语言模型（LLMs）可以解决的任务类型的心理模型，将会非常有趣。

我预计我们会看到其他人工智能实验室，包括开放模型权重社区，开始用他们自己版本的模型复制这些结果，这些模型专门训练用于应用这种思维链推理方式。

原文链接：[Notes on OpenAI’s new o1 chain-of-thought models](https://simonwillison.net/2024/Sep/12/openai-o1/)

本文使用 [🐝 A `C(Collect) -> T(Transform) -> P(Publish)` automation workflow for content creator.](https://github.com/qddegtya/r) 全自动采集 - 翻译 - 发布
